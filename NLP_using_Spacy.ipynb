{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ben-Ogega/Machine-Learning-Projects/blob/master/NLP_using_Spacy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "defA-5r4L80r"
      },
      "source": [
        "## Sentiment Analysis in Python\n",
        "### In this notebook we will be doing some Natural Language Processing (NLP) using Python, NLTK, and Spacy\n",
        "\n",
        "\n",
        "\n",
        "#### Implement **NLP in spaCy**\n",
        "\n",
        "1.   Customize and extend built-in functionalities in spaCy\n",
        "2.   Perform basic statistical analysis on a text\n",
        "3.   Create a **pipeline** to process **unstructured text**\n",
        "4.   Parse a sentence and extract meaningful insights from it\n",
        "5.   I refer from this [site](https://realpython.com/natural-language-processing-spacy-python/)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NLP is a subfield of artificial intelligence, and it’s all about allowing computers to comprehend human language.\n",
        "> NLP involves **Analyzing, Quantifying, Understanding, and Deriving** meaning from natural languages. Read more [here](https://realpython.com/natural-language-processing-spacy-python/)\n",
        "\n",
        "Examples of NLP applications include:\n",
        "\n",
        "1.   BERT from Google\n",
        "\n",
        "2.  GPT family from OpenAI\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lQ-anjDcZIYj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NLP helps you extract insights from unstructured text and has many use cases, such as:\n",
        "\n",
        "Automatic summarization\n",
        "\n",
        "> **Named-entity recognition**\n",
        "\n",
        "> **Question answering systems**\n",
        "\n",
        "> **Sentiment analysis**"
      ],
      "metadata": {
        "id": "F4Zjmwn7aI26"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation of spaCy"
      ],
      "metadata": {
        "id": "T0f-RmVUbaUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMmCzUP-bZZx",
        "outputId": "3760a63b-8f76-467f-dbda-e16d778ae8b9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wv7azQHPNQ-T"
      },
      "source": [
        "## Step 0. Read in Data and NLTK Basics"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The default model for the English language is designated as **en_core_web_sm**. Since the models are quite large, it’s best to **install them separately**— *including all languages in one package would make the download too massive.*"
      ],
      "metadata": {
        "id": "7c4qntF7cBGW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Spacy"
      ],
      "metadata": {
        "id": "P2IyulpacSZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp #is a callable spacy object"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YX0SJpvLcr5a",
        "outputId": "659aa463-bee5-44b6-e028-6b5714201d41"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.lang.en.English at 0x79d01b1c7f10>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # To start processing my input, I construct a Doc object.\n",
        " # A Doc object is a sequence of Token objects represneting a lexical token.\n",
        " # A token is an individual object ie word, punctuation, symbol, whitespace\n",
        "\n",
        "introduction_doc = nlp(\"This tutorial is about Natural Language Processing in spaCy.\")\n",
        "type(introduction_doc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDpHDs2gde0F",
        "outputId": "f72c6282-0c71-4077-f903-81706ba29d38"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.tokens.doc.Doc"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate tokens from the Doc\n",
        "tokens = [token.text for token in introduction_doc]\n",
        "tokens[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YepBIJTgiH_a",
        "outputId": "0d754d74-8f6d-49b2-8ba0-ab724b7d294f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We can also read from a file"
      ],
      "metadata": {
        "id": "MlcvsNd3imwE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import pathlib\n",
        "# file_name = \"introduction.txt\"\n",
        "# introduction_doc = nlp(pathlib.Path(file_name).read_text(encoding=\"utf-8\"))\n",
        "# print ([token.text for token in introduction_doc])"
      ],
      "metadata": {
        "id": "mJ7xsyOHifz0"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentence Detection\n",
        "Sentence detection is the process of locating where **sentences start** and **end in a given text**.\n",
        "\n",
        "This allows us **to divide a text into linguistically meaningful units.**"
      ],
      "metadata": {
        "id": "xkJjJXw-iz2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "about_text = (\n",
        "...     \"Gus Proto is a Python developer currently\"\n",
        "...     \" working for a London-based Fintech\"\n",
        "...     \" company. He is interested in learning\"\n",
        "...     \" Natural Language Processing.\"\n",
        "... )\n",
        "about_doc = nlp(about_text)\n",
        "sentences = list(about_doc.sents) # .sents property is used to extract sentences from the Doc object\n",
        "len(sentences)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWZ5pVO1jSc_",
        "outputId": "610e7796-cfaa-4e1d-886f-d45c3e9ec2b8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the first 5 token span\n",
        "for sentence in sentences:\n",
        "   print(f\"{sentence[:5]}...\")\n",
        "  #  print(type(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckjDlVYYjtDT",
        "outputId": "91e7d4cc-d8a6-492b-88f0-e61aa3fd24bb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gus Proto is a Python...\n",
            "He is interested in learning...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LOj_iBtNmFY"
      },
      "source": [
        "## **Tokens** in **spaCy**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building the Doc container involves **tokenizing the text**. The process of **tokenization breaks a text down into its basic units**—or **tokens**—which are represented in spaCy as **Token objects**."
      ],
      "metadata": {
        "id": "7C4EDqrlmncA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "about_text = ('''Gus Proto is a Python developer currently\n",
        "              working for a London-based Fintech\n",
        "               company. He is interested in learning\n",
        "                Natural Language Processing.''')\n",
        "about_doc = nlp(about_text)\n",
        "\n",
        "for token in about_doc:\n",
        "  print (token, token.idx)\n",
        "  # \"\"\"the .idx attribute, which represents\n",
        "  # the starting position of the token in the original text\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpLXVSJbzpQa",
        "outputId": "644dbd0f-d3f8-4eb2-b56b-c40210bc4814"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gus 0\n",
            "Proto 4\n",
            "is 10\n",
            "a 13\n",
            "Python 15\n",
            "developer 22\n",
            "currently 32\n",
            "\n",
            "               42\n",
            "working 57\n",
            "for 65\n",
            "a 69\n",
            "London 71\n",
            "- 77\n",
            "based 78\n",
            "Fintech 84\n",
            "\n",
            "                91\n",
            "company 107\n",
            ". 114\n",
            "He 116\n",
            "is 119\n",
            "interested 122\n",
            "in 133\n",
            "learning 136\n",
            "\n",
            "                 144\n",
            "Natural 161\n",
            "Language 169\n",
            "Processing 178\n",
            ". 188\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stop Words"
      ],
      "metadata": {
        "id": "wxkL6H1XpOSZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stop words are typically defined as the most common words in a language. In the English language, some examples of stop words are **the**, **are**, **but, and they**. Most sentences need to contain stop words in order to be **full sentences that make grammatical sense.**\n",
        "\n",
        "With NLP, **stop words are generally removed because they aren’t significant,** and they heavily distort any word frequency analysis. spaCy stores a list of stop words for the English language"
      ],
      "metadata": {
        "id": "ObFB0_H6pUN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
        "len(spacy_stopwords)\n",
        "\n",
        "for stop_word in list(spacy_stopwords)[:10]:\n",
        "  print(stop_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7LGJYqtoxJw",
        "outputId": "2e9d1aab-7fa4-4e40-d0a9-9212f3a27b9f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "back\n",
            "really\n",
            "this\n",
            "nevertheless\n",
            "'ll\n",
            "amount\n",
            "an\n",
            "would\n",
            "however\n",
            "why\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removing **stop words** from the input text by making use of the .**is_stop** attribute of each token"
      ],
      "metadata": {
        "id": "DLpmfdj-rbY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "custom_about_text = (\n",
        " \"\"\"Ben Ogega is a teacher and Mechanical engineering trainer currently\n",
        "working for a Nairobi-based  Road Safety\n",
        "company. He is interested in learning\n",
        "Natural Language Processing.\"\"\")\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "about_doc = nlp(custom_about_text)\n",
        "print([token for token in about_doc if not token.is_stop])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xWcooDwrnB3",
        "outputId": "f434a4d0-89e1-4b3b-9a88-e5fd16dbc589"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Ben, Ogega, teacher, Mechanical, engineering, trainer, currently, \n",
            ", working, Nairobi, -, based,  , Road, Safety, \n",
            ", company, ., interested, learning, \n",
            ", Natural, Language, Processing, .]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lemmatization**\n",
        "**Lemmatization** is the process of **reducing inflected forms** of a word while still ensuring that the reduced form belongs to the language. This **reduced form, or root word**, is called a **lemma**."
      ],
      "metadata": {
        "id": "uwkUBecSsqyc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, **organizes, organized and organizing** are all forms of **organize.** Here, **organize is the lemma**."
      ],
      "metadata": {
        "id": "On2RWOq3s9Vh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conference_help_text = (\"\"\"Ben Ogega is a teacher and Mechanical engineering\n",
        "trainer currently\n",
        "working for a Nairobi-based  Road Safety\n",
        "company. He is interested in learning\n",
        "Natural Language Processing.\"\"\")\n",
        "\n",
        "conference_help_doc = nlp(conference_help_text)\n",
        "for token in conference_help_doc:\n",
        "  if str(token) != str(token.lemma_):\n",
        "    print(f\"{str(token):>20} : {str(token.lemma_)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WP8odjsptN0h",
        "outputId": "04eb1edc-27f5-453c-eb63-33c07370fde7"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  is : be\n",
            "          Mechanical : mechanical\n",
            "             working : work\n",
            "               based : base\n",
            "                  He : he\n",
            "                  is : be\n",
            "            learning : learn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Frequency\n",
        "We can now convert a given text into **tokens** and **perform statistical** analysis on it."
      ],
      "metadata": {
        "id": "tSPyRUaduNG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "# nlp = spacy.load(\"en_core_web_sm\")\n",
        "complete_text = (\"\"\"Ben Ogega is a mechanical engineer witha penchant for\n",
        " programming in Python currently\n",
        "working for a Nairobi-based Road Safety company. He is\n",
        "interested in learning Natural Language Processing.\n",
        "He hopes to one day attend a developer conference in his city.\n",
        "If he gets anooportunity he will give his keynote speech a title\n",
        " \"Applications of Natural\n",
        "Language Processing in Road Safety and Defensive Driving\".\n",
        "He purposes to start organizing local Python meetups and several\n",
        "internal talks at his workplace. Ben is also an avid reader.\n",
        "His new found passion is in Machine Learning especially the  \"Use\n",
        "cases of Natural Language Processing in in Road Safety and Defensive Driving.\n",
        "Apart from his work, he is very passionate about music and history.\n",
        "He hopes to learn how to play a guiter.\"\"\")\n"
      ],
      "metadata": {
        "id": "a4FTw2p1ucpn"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(complete_text[:25])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03vDnl8CuxFX",
        "outputId": "564a775f-5552-4e91-cda2-7e00dffd49a4"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ben Ogega is a mechanical\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "complete_doc = nlp(complete_text)\n",
        "\n",
        "words = [token.text for token in complete_doc\n",
        "if not token.is_stop and not token.is_punct\n",
        " ]\n",
        "\n",
        "print(Counter(words).most_common(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJd-4zLputG3",
        "outputId": "78198a21-6fa6-4703-b804-c7e55289f8b9"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('\\n', 11), ('Road', 3), ('Safety', 3), ('Natural', 3), ('Language', 3)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing Stop words\n",
        "Counter([token.text for token in complete_doc if not token.is_punct]).most_common(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKaMHeArx6K0",
        "outputId": "616e46d7-8267-402e-f2ba-3a746f5fc152"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('\\n', 11), ('in', 7), ('is', 5), ('a', 5), ('He', 4)]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part-of-Speech Tagging\n",
        "**Part of speech** or POS is a grammatical role that explains how a **particular word is used in a sentence**. There are typically **eight parts of speech:**\n",
        "\n",
        "  1. **Noun**\n",
        "  2. **Pronoun**\n",
        "  3. **Adjective**\n",
        "  4. **Verb**\n",
        "  5. **Adverb**\n",
        "  6. **Preposition**\n",
        "  7. **Conjunction**\n",
        "  8. **Interjection**\n",
        "\n",
        "  POS tags are useful for assigning a **syntactic** category like **noun or verb** to each **word**"
      ],
      "metadata": {
        "id": "CxP-e4TFyYth"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8YteLa6Osv7"
      },
      "source": [
        "### Reading Data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\begin{equation}\n",
        "E = mc^2\n",
        "\\end{equation}"
      ],
      "metadata": {
        "id": "Tghdk_-SUc7o"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMbyXYtS0pbsdlRrwmkQ0J1",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}