{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ben-Ogega/Machine-Learning-Projects/blob/master/NLP_using_Spacy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "defA-5r4L80r"
      },
      "source": [
        "## Sentiment Analysis in Python\n",
        "### In this notebook we will be doing some Natural Language Processing (NLP) using Python, NLTK, and Spacy\n",
        "\n",
        "\n",
        "\n",
        "#### Implement **NLP in spaCy**\n",
        "\n",
        "1.   Customize and extend built-in functionalities in spaCy\n",
        "2.   Perform basic statistical analysis on a text\n",
        "3.   Create a **pipeline** to process **unstructured text**\n",
        "4.   Parse a sentence and extract meaningful insights from it\n",
        "5.   I refer from this [site](https://realpython.com/natural-language-processing-spacy-python/)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NLP is a subfield of artificial intelligence, and it’s all about allowing computers to comprehend human language.\n",
        "> NLP involves **Analyzing, Quantifying, Understanding, and Deriving** meaning from natural languages. Read more [here](https://realpython.com/natural-language-processing-spacy-python/)\n",
        "\n",
        "Examples of NLP applications include:\n",
        "\n",
        "1.   BERT from Google\n",
        "\n",
        "2.  GPT family from OpenAI\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lQ-anjDcZIYj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NLP helps you extract insights from unstructured text and has many use cases, such as:\n",
        "\n",
        "Automatic summarization\n",
        "\n",
        "> **Named-entity recognition**\n",
        "\n",
        "> **Question answering systems**\n",
        "\n",
        "> **Sentiment analysis**"
      ],
      "metadata": {
        "id": "F4Zjmwn7aI26"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation of spaCy"
      ],
      "metadata": {
        "id": "T0f-RmVUbaUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy"
      ],
      "metadata": {
        "id": "XMmCzUP-bZZx",
        "outputId": "3760a63b-8f76-467f-dbda-e16d778ae8b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wv7azQHPNQ-T"
      },
      "source": [
        "## Step 0. Read in Data and NLTK Basics"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The default model for the English language is designated as **en_core_web_sm**. Since the models are quite large, it’s best to **install them separately**— *including all languages in one package would make the download too massive.*"
      ],
      "metadata": {
        "id": "7c4qntF7cBGW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Spacy"
      ],
      "metadata": {
        "id": "P2IyulpacSZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp #is a callable spacy object"
      ],
      "metadata": {
        "id": "YX0SJpvLcr5a",
        "outputId": "659aa463-bee5-44b6-e028-6b5714201d41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.lang.en.English at 0x79d01b1c7f10>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # To start processing my input, I construct a Doc object.\n",
        " # A Doc object is a sequence of Token objects represneting a lexical token.\n",
        " # A token is an individual object ie word, punctuation, symbol, whitespace\n",
        "\n",
        "introduction_doc = nlp(\"This tutorial is about Natural Language Processing in spaCy.\")\n",
        "type(introduction_doc)\n"
      ],
      "metadata": {
        "id": "TDpHDs2gde0F",
        "outputId": "f72c6282-0c71-4077-f903-81706ba29d38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.tokens.doc.Doc"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate tokens from the Doc\n",
        "tokens = [token.text for token in introduction_doc]\n",
        "tokens[0]"
      ],
      "metadata": {
        "id": "YepBIJTgiH_a",
        "outputId": "0d754d74-8f6d-49b2-8ba0-ab724b7d294f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We can also read from a file"
      ],
      "metadata": {
        "id": "MlcvsNd3imwE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import pathlib\n",
        "# file_name = \"introduction.txt\"\n",
        "# introduction_doc = nlp(pathlib.Path(file_name).read_text(encoding=\"utf-8\"))\n",
        "# print ([token.text for token in introduction_doc])"
      ],
      "metadata": {
        "id": "mJ7xsyOHifz0"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentence Detection\n",
        "Sentence detection is the process of locating where **sentences start** and **end in a given text**.\n",
        "\n",
        "This allows us **to divide a text into linguistically meaningful units.**"
      ],
      "metadata": {
        "id": "xkJjJXw-iz2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "about_text = (\n",
        "...     \"Gus Proto is a Python developer currently\"\n",
        "...     \" working for a London-based Fintech\"\n",
        "...     \" company. He is interested in learning\"\n",
        "...     \" Natural Language Processing.\"\n",
        "... )\n",
        "about_doc = nlp(about_text)\n",
        "sentences = list(about_doc.sents) # .sents property is used to extract sentences from the Doc object\n",
        "len(sentences)\n"
      ],
      "metadata": {
        "id": "tWZ5pVO1jSc_",
        "outputId": "610e7796-cfaa-4e1d-886f-d45c3e9ec2b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the first 5 token span\n",
        "for sentence in sentences:\n",
        "   print(f\"{sentence[:5]}...\")\n",
        "  #  print(type(sentence))"
      ],
      "metadata": {
        "id": "ckjDlVYYjtDT",
        "outputId": "91e7d4cc-d8a6-492b-88f0-e61aa3fd24bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gus Proto is a Python...\n",
            "He is interested in learning...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LOj_iBtNmFY"
      },
      "source": [
        "Import Python Modules and Libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KpLXVSJbzpQa"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8YteLa6Osv7"
      },
      "source": [
        "### Reading Data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\begin{equation}\n",
        "E = mc^2\n",
        "\\end{equation}"
      ],
      "metadata": {
        "id": "Tghdk_-SUc7o"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMt7CszHQHYYLIwxreOXNRD",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}